{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This file does below:\n",
    "1. Import dataset and generate/add features\n",
    "2. Run different classifier models and predict the authenticity of a randomly selected article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Import libraries & Packages\n",
    "\n",
    "# Data structure and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import colorsys\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data (balanced_data.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = os.path.join('data', 'balanced_data.csv')\n",
    "total_df = pd.read_csv(path, usecols=[1,2,3,5,6])\n",
    "total_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_sites = list(set(list(total_df['source'][total_df.authenticity == 1])))\n",
    "t_sites = list(set(list(total_df['source'][total_df.authenticity == 0])))\n",
    "f = len(t_sites)\n",
    "t = len(f_sites)\n",
    "\n",
    "print(\"The number of total real news websites scraped is {} and that of fake ones is {}\".format(t,f))\n",
    "print('-'*100)\n",
    "print(\"Real news websites scraped are:\\n{}\".format(t_sites))\n",
    "print('-'*100)\n",
    "print(\"Fake news websites scraped are:\\n{}\".format(f_sites))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import additional features\n",
    "* Author exists (1) / not exists (0)\n",
    "* captital rates of title text, normalized to the average of caprate_title\n",
    "* Rate of exaggerating punctuations [!,?,:,-] in title text, normalized to the average of exagg_puct_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = os.path.join('data', 'additional_features.csv')\n",
    "addfeat_df = pd.read_csv(path, usecols=[1,2,3])\n",
    "addfeat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the whole dataset into train, cv, test datasets \n",
    "\n",
    "* X_( ) = text of article body, before processed\n",
    "* af_( ) = additional features, already processed\n",
    "* Combining X_ and af_ first, and split the data together, and separate them again so that text can be processed separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # define training, cross-validation and testing sets\n",
    "\n",
    "Y = total_df['authenticity']\n",
    "addfeat_df['text'] = total_df.text\n",
    "addfeat_df.head()\n",
    "Xaf = addfeat_df\n",
    "Xaf_train, Xaf_cvt, Y_train, Y_cvt = train_test_split(Xaf, Y, test_size=0.3, random_state=42)\n",
    "Xaf_cv, Xaf_test, Y_cv, Y_test = train_test_split(Xaf_cvt, Y_cvt, test_size= 0.6, random_state=42)\n",
    "X_train, af_train = Xaf_train.text, Xaf_train.drop('text', axis = 1)\n",
    "X_cv, af_cv = Xaf_cv.text, Xaf_cv.drop('text', axis = 1)\n",
    "X_test, af_test = Xaf_test.text, Xaf_test.drop('text', axis = 1)\n",
    "\n",
    "A = X_train.shape\n",
    "B = X_cv.shape\n",
    "C = X_test.shape\n",
    "D = af_train.shape\n",
    "E = af_cv.shape\n",
    "F = af_test.shape\n",
    "\n",
    "\n",
    "print(\"Shape of X_train is {}, X_cv is {}, X_test is {}, af_train is {}, af_cv is {}, af_test is {}\"\n",
    "      .format(A,B,C,D,E,F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate features from article body text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize_stemmer(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # option to include punctuation or not\n",
    "    #tokens = [i for i in tokens if i not in string.punctuation]\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Use CountVectorizer to generate features¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import and instantiate CountVectorizer (with the default parameters)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# instantiate the vectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# learn training data vocabulary, then use it to create a document-term matrix\n",
    "vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_tot = vect.transform(X_train)\n",
    "X_test_tot = vect.transform(X_test)\n",
    "X_cv_tot = vect.transform(X_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Use TF-IDF to generate features for the text body of news¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "my_additional_stop_words = ['abc', 'nbc', 'npr', 'cnn', 'reuters', 'fox', \n",
    "                            'bbc', 'cbs', 'newyorker', 'msnbc', 'politico', 'nytimes',\n",
    "                            'sputniknews', 'lastdeplorables', 'readconservatives', 'wordpress',\n",
    "                            'infostormer', 'Americannews', 'ABCnews', 'nationonenews', 'majorthoughts',\n",
    "                            'interestingdailynews', 'donaldtrumppotus45', 'newsbbc', 'beforeitsnews', \n",
    "                            'krbcnews', 'Conservativedailypost', 'thedcgazette', 'Americanoverlook', \n",
    "                            'CivicTribune', 'openmagazines', 'politicono', 'bizstandardnews', 'president45donaldtrump',\n",
    "                            'nbc', 'AmericanFlavor', 'prntly', 'bipartisanreport', 'americanfreepress', \n",
    "                            'ladylibertysnews', 'politicalo', 'now8news', '24wpn', 'pamelageller', \n",
    "                            'ddsnewstrend', 'Bighairynews', 'redcountry', 'newswithviews', 'Clashdaily', \n",
    "                            'aurora-news', 'nephef', 'local31news', 'realnewsrightnow', 'reagancoalition',\n",
    "                            'reuter', 'sputnik', \n",
    "                            'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday' ]\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize_stemmer,stop_words=my_stop_words,ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_tot = tfidf.transform(X_train)\n",
    "X_test_tot = tfidf.transform(X_test)\n",
    "X_cv_tot = tfidf.transform(X_cv)\n",
    "\n",
    "X_train_tot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, SelectKBest, f_classif, chi2, SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select top 10% features\n",
    "# selector = SelectPercentile(f_classif, percentile = 10) \n",
    "\n",
    "# select top 25000 features\n",
    "#selector = SelectKBest(chi2, k = 10000) \n",
    "\n",
    "\n",
    "# select from model\n",
    "lsvc = LinearSVC(C=9000, penalty=\"l1\", dual=False)#.fit(X_train_tot, Y_train)\n",
    "selector = SelectFromModel(lsvc, prefit=False)\n",
    "\n",
    "\n",
    "selector.fit(X_train_tot, Y_train)\n",
    "X_train_selected = selector.transform(X_train_tot)\n",
    "X_test_selected = selector.transform(X_test_tot)\n",
    "X_cv_selected = selector.transform(X_cv_tot)\n",
    "\n",
    "print(\"X_train_selected shape: \", X_train_selected.shape)\n",
    "print(\"X_test_selected shape: \", X_test_selected.shape)\n",
    "print(\"X_cv_selected shape: \", X_cv_selected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain seleted feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For total (w/o additional features)\n",
    "feat_names_tfidf = tfidf.get_feature_names()\n",
    "# For selected features (w/o additional features)\n",
    "feat_names_sel = selector.transform(feat_names_tfidf)\n",
    "feat_names_sel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat_names_sel_list = feat_names_sel.flatten().tolist()\n",
    "feat_names_sel_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize top tf-idf features for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_tfidf_features(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_features_in_doc(x_train_dtm, features, row_id, top_n=25):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    row = np.squeeze(x_train_dtm[row_id].toarray())\n",
    "    return top_tfidf_features(row, features, top_n)\n",
    "\n",
    "def top_mean_features(x_train_dtm, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = x_train_dtm[grp_ids].toarray()\n",
    "    else:\n",
    "        D = x_train_dtm.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_features(tfidf_means, features, top_n)\n",
    "\n",
    "def top_features_by_class(x_train_dtm, y_train, features, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = []\n",
    "    labels = np.unique(y_train)\n",
    "    for label in labels:\n",
    "        ids = np.where(y_train==label)\n",
    "        features_df = top_mean_features(x_train_dtm, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        features_df.label = label\n",
    "        dfs.append(features_df)\n",
    "    return dfs\n",
    "\n",
    "def plot_tfidf_classfeats_h(dfs):\n",
    "    ''' Plot the data frames returned by the function plot_tfidf_classfeats(). '''\n",
    "    width = 0.5\n",
    "    fig = plt.figure(figsize=(12, 5), facecolor=\"w\")\n",
    "    x = np.arange(len(dfs[0]))\n",
    "    for i, df in enumerate(dfs):\n",
    "        ax = fig.add_subplot(1, len(dfs), i+1)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.set_frame_on(False)\n",
    "        ax.get_xaxis().tick_bottom()\n",
    "        ax.get_yaxis().tick_left()\n",
    "        ax.set_xlabel(\"Mean TF-IDF Score\", labelpad=16, fontsize=12.5)\n",
    "        ax.set_title(\"Class = \" + str(df.label), fontsize=13.5)\n",
    "        ax.ticklabel_format(axis='x', fontsize=12.5)\n",
    "        ax.barh(x, df.tfidf, width, align='center', color='indigo')\n",
    "        ax.set_yticks(x)\n",
    "        ax.set_ylim([-1, x[-1]+1])\n",
    "        yticks = ax.set_yticklabels(df.feature,fontsize=12.5)\n",
    "        plt.subplots_adjust(bottom=0.09, right=0.97, left=0.15, top=0.95, wspace=0.52)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./figures/topfeatures_tfidf.png', transparent=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_features = top_features_by_class(X_train_selected, Y_train, feat_names_sel_list,top_n=10)\n",
    "plot_tfidf_classfeats_h(top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set train, test, cv data (choose one method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "def concat_3features(df):\n",
    "    author = np.array([[x] for x in df.author])\n",
    "    caprate = np.array([[x] for x in df.caprate_title])\n",
    "    exagg =  np.array([[x] for x in df.exagg_puct_title])\n",
    "    return  np.concatenate((author, caprate, exagg), axis=1)\n",
    "\n",
    "# add additional feature names\n",
    "feat_names_add = ['author', 'title_capitalization', 'exclamation_in_title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. use all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_tot_add = sp.sparse.hstack((X_train_tot, concat_3features(af_train)))\n",
    "X_cv_tot_add = sp.sparse.hstack((X_cv_tot, concat_3features(af_cv)))\n",
    "X_test_tot_add = sp.sparse.hstack((X_test_tot, concat_3features(af_test)))\n",
    "print(\"X_train_tot_add shape: \", X_train_tot_add.shape)\n",
    "print(\"X_test_tot_add shape: \", X_test_tot_add.shape)\n",
    "print(\"X_cv_tot_add shape: \", X_cv_tot_add.shape)\n",
    "\n",
    "X_train_dtm = scaler.fit_transform(X_train_tot_add)\n",
    "X_test_dtm = scaler.fit_transform(X_test_tot_add)\n",
    "X_cv_dtm = scaler.fit_transform(X_cv_tot_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat_names = feat_names_tfidf + feat_names_add \n",
    "feat_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. use selected features from text body of news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_dtm = scaler.fit_transform(X_train_selected)\n",
    "X_test_dtm = scaler.fit_transform(X_test_selected)\n",
    "X_cv_dtm = scaler.fit_transform(X_cv_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat_names = feat_names_sel_list\n",
    "feat_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. use selected and additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_sel_add = sp.sparse.hstack((X_train_selected, concat_3features(af_train)))\n",
    "X_cv_sel_add = sp.sparse.hstack((X_cv_selected, concat_3features(af_cv)))\n",
    "X_test_sel_add = sp.sparse.hstack((X_test_selected, concat_3features(af_test)))\n",
    "print(\"X_train_sel_add shape: \", X_train_sel_add.shape)\n",
    "print(\"X_test_sel_add shape: \", X_test_sel_add.shape)\n",
    "print(\"X_cv_sel_add shape: \", X_cv_sel_add.shape)\n",
    "\n",
    "# scaler.fit(X_train_sel_add)\n",
    "# X_train_dtm = scaler.transform(X_train_sel_add)\n",
    "# X_test_dtm = scaler.transform(X_test_sel_add)\n",
    "# X_cv_dtm = scaler.transform(X_cv_sel_add)\n",
    "X_train_dtm = X_train_sel_add\n",
    "X_test_dtm = X_test_sel_add\n",
    "X_cv_dtm = X_cv_sel_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat_names = feat_names_sel_list + feat_names_add \n",
    "feat_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test different machine learning classifier models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# validation test & Learning curve\n",
    "\n",
    "def valid_test(model, param, param_candidates):\n",
    "    \n",
    "#     candidates = np.logspace(-7, 7, 10) # for alpha, C \n",
    "#     candidates = [1,2,3,4,5,6,7,8,9,10]# for integers\n",
    "    train_scores, valid_scores = validation_curve(model, X_cv_dtm, Y_cv, param, param_candidates)\n",
    "    avg_ts, avg_vs = train_scores.mean(axis = 1), valid_scores.mean(axis = 1)\n",
    "    sd_ts, sd_vs = train_scores.std(axis = 1), valid_scores.std(axis = 1)\n",
    "    vs_max_ix = np.argmax(avg_vs)\n",
    "    best = param_candidates[vs_max_ix]\n",
    "    print('The best {} value for {} is {}'.format(param, model, best))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.ylim([0,1.1])\n",
    "    plt.title('Validation Test')\n",
    "    plt.ylabel('Scores')\n",
    "    plt.xlabel('Candidate {} index'.format(param))\n",
    "    plt.plot(range(10), avg_ts, label = 'Training Scores')\n",
    "    plt.plot(range(10), avg_vs, label = 'Cross validation Scores')\n",
    "    plt.legend(loc = 'best')\n",
    "    return best\n",
    "\n",
    "def Learning_curve(model):\n",
    "    train_sizes = [50,100,150,200,250,300,350,400]\n",
    "    train_sizes, train_scores, test_scores = learning_curve(model, X_train_dtm, Y_train, train_sizes=train_sizes)   \n",
    "    train_scores_mean, test_scores_mean = train_scores.mean(axis = 1), test_scores.mean(axis = 1)\n",
    "    train_scores_std, test_scores_std = train_scores.std(axis = 1), test_scores.std(axis = 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.ylim([0,1.1])\n",
    "    plt.title('Learning Curve',fontsize=15)\n",
    "    plt.ylabel('Scores',fontsize=13.5)\n",
    "    plt.xlabel('Training Size',fontsize=13.5)\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training Score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-Validation Score\")\n",
    "    plt.legend(loc = 'best')\n",
    "    plt.savefig('./figures/learning_curve.png', transparent=True)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_roc_curve(model):\n",
    "    y_pred_prob = model.predict_proba(X_test_dtm)\n",
    "    fpr, tpr, thrs = roc_curve(Y_test, y_pred_prob[:,1])\n",
    "        \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr, label='Logistic Regression', color='darkorange',lw = 3)\n",
    "    plt.xlabel('False Positive Rate',fontsize=13.5)\n",
    "    plt.ylabel('True Positive Rate',fontsize=13.5)\n",
    "    plt.title('ROC Curve',fontsize=15)\n",
    "    plt.savefig('./figures/roc_curve.png', transparent=True)\n",
    "    plt.show()\n",
    "    print('AUC is {}'.format(auc(fpr, tpr)))\n",
    "\n",
    "def plot_precision_recall(model):\n",
    "    y_pred_dec = model.decision_function(X_test_dtm)\n",
    "    precision, recall, thresholds = precision_recall_curve(Y_test, y_pred_dec)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot([0, 1], [1, 0],'k--')\n",
    "    plt.plot(recall, precision, label='Logistic Regression', color='teal',lw = 3)\n",
    "    plt.xlabel('Recall',fontsize=13.5)\n",
    "    plt.ylabel('Precision',fontsize=13.5)\n",
    "    plt.title('Recall-Precision Curve',fontsize=15)\n",
    "    plt.savefig('./figures/recall_precision_curve.png', transparent=True)\n",
    "    plt.show()\n",
    "\n",
    "def print_scores(model, y_test_pred):\n",
    "    print('Train score is {}'.format(model.score(X_train_dtm, Y_train)))\n",
    "    print('Test score is {}'.format(model.score(X_test_dtm, Y_test)))\n",
    "    print('F1 score is {}'.format(f1_score(Y_test,y_test_pred)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_C = valid_test(LogisticRegression(), \"C\", np.logspace(2, 4, 10))\n",
    "LR = LogisticRegression(C = best_C)\n",
    "#LR = LogisticRegression()\n",
    "Learning_curve(LR)\n",
    "%time LR.fit(X_train_dtm, Y_train)\n",
    "Y_test_pred_LR = LR.predict(X_test_dtm)\n",
    "\n",
    "# polynomial_features = PolynomialFeatures(degree=2,include_bias=False)\n",
    "# pipeline = Pipeline([(\"polynomial_features\", polynomial_features), (\"linear_regression\", LR)])\n",
    "# Learning_curve(pipeline)\n",
    "# # Learning_curve(LR)\n",
    "\n",
    "# pipeline.fit(X_train, Y_train)\n",
    "# Y_test_pred_LR = pipeline.predict(X_test)\n",
    "# print('Train score is {}'.format(pipeline.score(X_train, Y_train)))\n",
    "# print('Cross validation score is {}'.format(pipeline.score(X_cv, Y_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot precision recall curve\n",
    "plot_precision_recall(LR)\n",
    "# plot ROC curve\n",
    "plot_roc_curve(LR)\n",
    "# print scores\n",
    "print_scores(LR, Y_test_pred_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize  top features for each class using coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_coefficients(classifier, feature_names, top_features=20):\n",
    " coef = classifier.coef_.ravel()\n",
    " top_positive_coefficients = np.argsort(coef)[-top_features:]\n",
    " top_negative_coefficients = np.argsort(coef)[:top_features]\n",
    " top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
    " # create plot\n",
    " plt.figure(figsize=(15, 5))\n",
    " colors = ['green' if c < 0 else 'red' for c in coef[top_coefficients]]\n",
    " plt.bar(np.arange(2 * top_features), coef[top_coefficients], color=colors)\n",
    " feature_names = np.array(feature_names)\n",
    " plt.xticks(np.arange(1, 1 + 2 * top_features), feature_names[top_coefficients], \n",
    "            rotation=60, ha='right', fontsize=12)\n",
    " plt.ylabel('Coefficient',fontsize=12)\n",
    " plt.tight_layout()\n",
    " plt.savefig('./figures/topfeatures_class.png', transparent=True)\n",
    " plt.show()\n",
    "    \n",
    "plot_coefficients(LR, feat_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(label):\n",
    "    x = [x for x in range(LR.coef_.size)]\n",
    "\n",
    "    LR.coef_.sort()\n",
    "    y = LR.coef_\n",
    "    \n",
    "    f, ax = plt.subplots(1,1,figsize=(10,6))\n",
    "    ax.set_xlim([0,LR.coef_.size])\n",
    "    ax.set_ylabel('Coefficient',fontsize=13.5)\n",
    "    ax.set_xlabel('Number of Features',fontsize=13.5)\n",
    "    ax.bar(x,y[0],color='b',edgecolor='none',width=1.0)\n",
    "    ax.set_title('Feature Importance',fontsize=15)\n",
    "    file_name = './figures/' + label + '_feature_selection_coef.png'\n",
    "    plt.savefig(file_name, transparent=True)\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. before feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# too slow\n",
    "%time plot_feature_importance('before')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. after feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time plot_feature_importance('after')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional information (feature selection vs. prediction rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import feature_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform = feature_selection.SelectPercentile(feature_selection.f_classif)\n",
    "clf = Pipeline([('anova', transform), ('LR', LogisticRegression(C = best_C))])\n",
    "\n",
    "score_means = list()\n",
    "score_stds = list()\n",
    "percentiles = (1, 3, 6, 10, 15, 20, 30, 40, 60, 80, 100)\n",
    "\n",
    "for percentile in percentiles:\n",
    "    clf.set_params(anova__percentile=percentile)\n",
    "    this_scores = cross_val_score(clf, X_cv_dtm, Y_cv)\n",
    "    score_means.append(this_scores.mean())\n",
    "    score_stds.append(this_scores.std())\n",
    "    \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.ylim([0,1])\n",
    "plt.errorbar(percentiles, score_means, np.array(score_stds))\n",
    "\n",
    "plt.title('Performance of the LR-Anova varying the percentile of features selected', fontsize=15)\n",
    "plt.xlabel('Percentile', fontsize=13.5)\n",
    "plt.ylabel('Prediction rate', fontsize=13.5)\n",
    "plt.savefig('./figures/prediction_percentile.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X_train_dtm, Y_train)\n",
    "# display the relative importance of each attribute\n",
    "#print(model.feature_importances_)\n",
    "df = pd.DataFrame(model.feature_importances_, columns = ['importance'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.xlim([0,df.index.size])\n",
    "plt.bar(df.index, df.importance, color=\"b\",edgecolor='none')\n",
    "plt.title('Feature importance', fontsize=15)\n",
    "plt.xlabel('Features', fontsize=13.5)\n",
    "plt.ylabel('Importances', fontsize=13.5)\n",
    "#plt.savefig('./figures/feature_importances.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Neural networking (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_alpha = valid_test(MLPClassifier(), 'alpha', np.logspace(-5, -1, 10))\n",
    "MLP = MLPClassifier(alpha=best_alpha, hidden_layer_sizes=(10, 10, 10), random_state=1, max_iter=5000)\n",
    "# MLP = MLPClassifier(alpha=1e-05, hidden_layer_sizes=(12, 12, 12), random_state=1, max_iter=5000)\n",
    "MLP.fit(X_train_dtm, Y_train)\n",
    "Learning_curve(MLP)\n",
    "Y_test_pred_MLP = MLP.predict(X_test_dtm)\n",
    "\n",
    "# polynomial_features = PolynomialFeatures(degree=2,include_bias=False)\n",
    "# pipeline = Pipeline([(\"polynomial_features\", polynomial_features), (\"MLP_Classifier\", MLP)])\n",
    "# pipeline.fit(X_train, Y_train)\n",
    "# Learning_curve(pipeline)\n",
    "# Y_test_pred_MLP = MLP.predict(X_test)\n",
    "\n",
    "# print('train score is {}'.format(pipeline.score(X_train, Y_train)))\n",
    "# print('cross validation score is {}'.format(pipeline.score(X_cv, Y_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot precision recall curve\n",
    "plot_precision_recall(MLP, X_test_dtm, Y_test)\n",
    "# plot ROC curve\n",
    "plot_roc_curve(MLP, X_test_dtm, Y_test)\n",
    "# print scores\n",
    "print_scores(MLP, X_train_dtm, Y_train, X_test_dtm, Y_test, Y_test_pred_MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_gamma = valid_test(SVC(), \"gamma\", np.logspace(-5, 5, 10))\n",
    "best_C = valid_test(SVC(gamma = best_gamma), \"C\", np.logspace(-5, 5, 10))\n",
    "svm = SVC(C = best_C, gamma = best_gamma)\n",
    "# svm = SVC(C = 3.6, gamma = 0.0016)\n",
    "svm.fit(X_train_dtm, Y_train)\n",
    "Learning_curve(svm)\n",
    "Y_test_pred_svm = svm.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot precision recall curve\n",
    "plot_precision_recall(svm, X_test_dtm, Y_test)\n",
    "# plot ROC curve\n",
    "#plot_roc_curve(svm, X_test_dtm, Y_test)\n",
    "# print scores\n",
    "print_scores(svm, X_train_dtm, Y_train, X_test_dtm, Y_test, Y_test_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_estimator = valid_test(RandomForestClassifier(), \"n_estimators\", [x for x in range(2,12)])\n",
    "RF = RandomForestClassifier(n_estimators=best_estimator)\n",
    "Learning_curve(RF)\n",
    "%time RF.fit(X_train_dtm, Y_train)\n",
    "Y_test_pred_RF = RF.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RF does not have decision_function\n",
    "#plot_precision_recall(RF, X_test_dtm, Y_test)\n",
    "\n",
    "# plot ROC curve\n",
    "plot_roc_curve(RF)\n",
    "# print scores\n",
    "print_scores(RF, Y_test_pred_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_alpha = valid_test(MultinomialNB(), \"alpha\", np.logspace(-10, 2, 10))\n",
    "nb = MultinomialNB(alpha = best_alpha)\n",
    "#nb = MultinomialNB()\n",
    "Learning_curve(nb)\n",
    "%time nb.fit(X_train_dtm, Y_train)\n",
    "Y_test_pred_nb = nb.predict(X_test_dtm)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nb does not have decision_function\n",
    "#plot_precision_recall(nb)\n",
    "\n",
    "# plot ROC curve\n",
    "plot_roc_curve(nb)\n",
    "# print scores\n",
    "print_scores(nb, Y_test_pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
