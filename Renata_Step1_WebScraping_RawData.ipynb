{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Raw Article Data for the Fake News Detection Project\n",
    "### This script is for the step no.1 of 3 steps process: <br>\n",
    "<font color = red>1. Web-scrap raw article data and re-format it into a pandas data structure </font><br>\n",
    "<font color = black>2. Extracting useful features from the raw data<br>\n",
    "3. Using the features, try different supervised learning to detect fake news</font>\n",
    "\n",
    "<br>\n",
    "### 30 'less reputable' and also known to be 'fake /extremly-biased' news organizations:\n",
    "* 'http://ABCnews.com.co'\n",
    "* 'http://bizstandardnews.com'\n",
    "* 'http://Bloomberg.ma'\n",
    "* 'http://70news.wordpress.com'\n",
    "* 'http://beforeitsnews.com'\n",
    "* 'http://ddsnewstrend.com'\n",
    "* 'http://thebostontribune.com/'\n",
    "* 'http://americanfreepress.net/'\n",
    "* 'http://www.bipartisanreport.com/'\n",
    "* 'http://aurora-news.us/'\n",
    "* 'http://conservativefighters.com/'\n",
    "* 'http://conservativespirit.com/'\n",
    "* 'http://conservative101.com/'\n",
    "* 'http://DrudgeReport.com.co'\n",
    "* 'http://NBCNews.com.co'\n",
    "* 'http://TrueTrumpers.com'\n",
    "* 'http://UndergroundNewsReport.com'\n",
    "* 'http://washingtonpost.com.co'\n",
    "* 'http://YourNewsWire.com'\n",
    "* 'http://cnn.com.de/'\n",
    "* 'http://rickwells.us/'\n",
    "* 'http://thedcgazette.com/'\n",
    "* 'http://donaldtrumppotus45.com/'\n",
    "* 'http://24wpn.com'\n",
    "* 'http://AmericanFlavor.news'\n",
    "* 'http://AmericanPresident.co'\n",
    "* 'http://AMPosts.com'\n",
    "* 'http://BB4SP.com'\n",
    "* 'http://BlueVisionPost.com'\n",
    "* 'http://CivicTribune.com'\n",
    "\n",
    "### 30 'reputable' and 'trusted' news organizations:\n",
    "* 'https://www.wsj.com/'\n",
    "* 'https://www.nytimes.com/'\n",
    "* 'http://www.bbc.com/news'\n",
    "* 'http://www.npr.org/sections/news/'\n",
    "* 'http://www.reuters.com/'\n",
    "* 'https://www.economist.com/'\n",
    "* 'https://www.apnews.com/'\n",
    "* 'http://www.cnn.com'\n",
    "* 'http://www.foxnews.com/'\n",
    "* 'http://www.politico.com/'\n",
    "* 'http://www.nbcnews.com/'\n",
    "* 'http://www.msnbc.com/'\n",
    "* 'http://www.cbsnews.com/'\n",
    "* 'http://www.huffingtonpost.com/'\n",
    "* 'http://www.bloomberg.com/'\n",
    "* 'http://abcnews.go.com/'\n",
    "* 'http://www.aljazeera.com/news/'\n",
    "* 'https://www.afp.com/en/news-hub'\n",
    "* 'http://www.newyorker.com/'\n",
    "* 'https://www.theguardian.com/'\n",
    "* 'http://www.telegraph.co.uk/'\n",
    "* 'http://www.zeit.de/english/index',\n",
    "* 'http://www.chicagotribune.com/'\n",
    "* 'http://www.vox.com/'\n",
    "* 'http://www.bostonherald.com/'\n",
    "* 'http://www.dailypress.com/'\n",
    "* 'http://www.detroitnews.com/'\n",
    "* 'https://www.ft.com/'\n",
    "* 'http://www.ibtimes.com/'\n",
    "* 'http://www.voanews.com/'\n",
    "\n",
    "\n",
    "\n",
    "### Reference:\n",
    "1. 'http://www.politifact.com/punditfact/article/2017/apr/20/politifacts-guide-fake-news-websites-and-what-they/'\n",
    "2. 'https://mediabiasfactcheck.com/'\n",
    "3. 'https://en.wikipedia.org/wiki/List_of_fake_news_websites'\n",
    "4. 'http://www.fakenewsai.com/'\n",
    "5. 'https://morningconsult.com/2016/12/07/poll-majority-find-major-media-outlets-credible/'\n",
    "6. 'http://www.pewresearch.org/fact-tank/2014/10/30/which-news-organization-is-the-most-trusted-the-answer-is-complicated/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the fake news and true news list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 30 fake news and 30 true news organizations\n"
     ]
    }
   ],
   "source": [
    "renata_fake_news = [put your own!!!]\n",
    "\n",
    "renata_true_news = ['http://www.telegraph.co.uk/', 'http://www.zeit.de/english/index',\n",
    "             'http://www.chicagotribune.com/', 'http://www.freep.com/', 'http://www.bostonherald.com/',\n",
    "             'http://www.dailypress.com/', 'http://www.detroitnews.com/', 'https://www.ft.com/', \n",
    "             'http://www.ibtimes.com/', 'http://www.voanews.com/']\n",
    "\n",
    "\n",
    "\n",
    "print(\"We have {} fake news and {} true news organizations\".format(len(fake_news), len(true_news)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import libraries and tools\n",
    "* downloading nltk can take some time\n",
    "* alternative of 'nltk spell check' is to use a libary called enchanted, but I failed to install it for some reasons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import newspaper \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article `download()` failed with 404 Client Error: Not Found for url: http://jp.wsj.com/news/economy/monetary-policy%20 on URL http://jp.wsj.com/news/economy/monetary-policy \n",
      "Article `download()` failed with 404 Client Error: Not Found for url: http://jp.wsj.com/news/economy/heard-on-the-street%20 on URL http://jp.wsj.com/news/economy/heard-on-the-street \n",
      "Article `download()` failed with 404 Client Error: Not Found for url: http://jp.wsj.com/news/world/north-america%20 on URL http://jp.wsj.com/news/world/north-america \n",
      "Article `download()` failed with 404 Client Error: Not Found for url: http://jp.wsj.com/news/world/asia-oceania%20 on URL http://jp.wsj.com/news/world/asia-oceania \n",
      "Article `download()` failed with 404 Client Error: Not Found for url: http://jp.wsj.com/news/types/j-chinas-world%20 on URL http://jp.wsj.com/news/types/j-chinas-world \n",
      "Article `download()` failed with 404 Client Error: Not Found for url: http://jp.wsj.com/news/types/j-shasetsu%20 on URL http://jp.wsj.com/news/types/j-shasetsu \n",
      "The total number of wsj articles is  326\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: http://www.bbc.com/news/video_and_audio on URL http://www.bbc.com/news/video_and_audio\n",
      "You must `download()` an article first!\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: http://newsvote.bbc.co.uk/email/news on URL http://newsvote.bbc.co.uk/go/news/int/story/services/-/email/news\n",
      "The total number of bbc articles is  411\n"
     ]
    }
   ],
   "source": [
    "def generate_raw_true_data(news_list, data_name):\n",
    "    col_names = [\"source\", \"title\", \"author\", \"text\"]\n",
    "    article_df = pd.DataFrame(columns = col_names)\n",
    "    \n",
    "    for news in news_list:\n",
    "        news_articles = newspaper.build(news, memoize_articles=False)\n",
    "        count = 0\n",
    "        for article in news_articles.articles:\n",
    "            try:\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                entry = pd.DataFrame([[article.url, article.title, article.authors, article.text]], columns=col_names)\n",
    "                article_df = article_df.append(entry)\n",
    "                count += 1\n",
    "            except:\n",
    "                pass\n",
    "         \n",
    "        print(\"The total number of \" + str(news_articles.brand) + \" articles is \", count)    \n",
    "    article_df.to_csv(data_name+\".csv\")\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_raw_fake_data(news_list, data_name):\n",
    "    col_names = [\"source\", \"title\", \"author\", \"text\"]\n",
    "    article_df = pd.DataFrame(columns = col_names)\n",
    "    final_news_list = []\n",
    "    final_article_number = {}\n",
    "    total_count = 0\n",
    "    for news in news_list:\n",
    "        try:\n",
    "            news_articles = newspaper.build(news, memoize_articles=False)\n",
    "            final_news_list += [news_articles]\n",
    "        except:\n",
    "            pass\n",
    "    print (final_news_list, len(final_news_list))\n",
    "    for news_articles in final_news_list:\n",
    "        if total_count < 1100:\n",
    "            count = 0\n",
    "            for article in news_articles.articles:\n",
    "                try:\n",
    "                    article.download()\n",
    "                    article.parse()\n",
    "                    entry = pd.DataFrame([[article.url, article.title, article.authors, article.text]], columns=col_names)\n",
    "                    text_vocab = set(w.lower() for w in entry.title if w.lower().isalpha()) \n",
    "                    article_df = article_df.append(entry)\n",
    "                    count += 1\n",
    "                    total_count += 1\n",
    "                except:\n",
    "                    pass\n",
    "            print(\"The total number of \" + str(news_articles.brand) + \" articles is \", count) \n",
    "            final_article_number[news_articles.brand] = count\n",
    "        else:\n",
    "            pass\n",
    "    print(final_article_number)\n",
    "    article_df.to_csv(data_name+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# true\n",
    "generate_raw_true_data(renata_true_news, 'renata_true_news_rawdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fake\n",
    "generate_raw_fake_data(renata_fake_news, 'renata_fake_news_rawdata')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
