{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Raw Article Data for the Fake News Detection Project\n",
    "### This script is for the step no.1 of 3 steps process: <br>\n",
    "<font color = red>1. Web-scrap raw article data and re-format it into a pandas data structure </font><br>\n",
    "<font color = black>2. Extracting useful features from the raw data<br>\n",
    "3. Using the features, try different supervised learning to detect fake news</font>\n",
    "\n",
    "<br>\n",
    "### 30 'less reputable' and also known to be 'fake /extremly-biased' news organizations:\n",
    "* 'http://ABCnews.com.co'\n",
    "* 'http://bizstandardnews.com'\n",
    "* 'http://Bloomberg.ma'\n",
    "* 'http://70news.wordpress.com'\n",
    "* 'http://beforeitsnews.com'\n",
    "* 'http://ddsnewstrend.com'\n",
    "* 'http://thebostontribune.com/'\n",
    "* 'http://americanfreepress.net/'\n",
    "* 'http://www.bipartisanreport.com/'\n",
    "* 'http://aurora-news.us/'\n",
    "* 'http://conservativefighters.com/'\n",
    "* 'http://conservativespirit.com/'\n",
    "* 'http://conservative101.com/'\n",
    "* 'http://DrudgeReport.com.co'\n",
    "* 'http://NBCNews.com.co'\n",
    "* 'http://TrueTrumpers.com'\n",
    "* 'http://UndergroundNewsReport.com'\n",
    "* 'http://washingtonpost.com.co'\n",
    "* 'http://YourNewsWire.com'\n",
    "* 'http://cnn.com.de/'\n",
    "* 'http://rickwells.us/'\n",
    "* 'http://thedcgazette.com/'\n",
    "* 'http://donaldtrumppotus45.com/'\n",
    "* 'http://24wpn.com'\n",
    "* 'http://AmericanFlavor.news'\n",
    "* 'http://AmericanPresident.co'\n",
    "* 'http://AMPosts.com'\n",
    "* 'http://BB4SP.com'\n",
    "* 'http://BlueVisionPost.com'\n",
    "* 'http://CivicTribune.com'\n",
    "\n",
    "### 30 'reputable' and 'trusted' news organizations:\n",
    "* 'https://www.wsj.com/'\n",
    "* 'https://www.nytimes.com/'\n",
    "* 'http://www.bbc.com/news'\n",
    "* 'http://www.npr.org/sections/news/'\n",
    "* 'http://www.reuters.com/'\n",
    "* 'https://www.economist.com/'\n",
    "* 'https://www.apnews.com/'\n",
    "* 'http://www.cnn.com'\n",
    "* 'http://www.foxnews.com/'\n",
    "* 'http://www.politico.com/'\n",
    "* 'http://www.nbcnews.com/'\n",
    "* 'http://www.msnbc.com/'\n",
    "* 'http://www.cbsnews.com/'\n",
    "* 'http://www.huffingtonpost.com/'\n",
    "* 'http://www.bloomberg.com/'\n",
    "* 'http://abcnews.go.com/'\n",
    "* 'http://www.aljazeera.com/news/'\n",
    "* 'https://www.afp.com/en/news-hub'\n",
    "* 'http://www.newyorker.com/'\n",
    "* 'https://www.theguardian.com/'\n",
    "* 'http://www.telegraph.co.uk/'\n",
    "* 'http://www.zeit.de/english/index',\n",
    "* 'http://www.chicagotribune.com/'\n",
    "* 'http://www.vox.com/'\n",
    "* 'http://www.bostonherald.com/'\n",
    "* 'http://www.dailypress.com/'\n",
    "* 'http://www.detroitnews.com/'\n",
    "* 'https://www.ft.com/'\n",
    "* 'http://www.ibtimes.com/'\n",
    "* 'http://www.voanews.com/'\n",
    "\n",
    "\n",
    "\n",
    "### Reference:\n",
    "1. 'http://www.politifact.com/punditfact/article/2017/apr/20/politifacts-guide-fake-news-websites-and-what-they/'\n",
    "2. 'https://mediabiasfactcheck.com/'\n",
    "3. 'https://en.wikipedia.org/wiki/List_of_fake_news_websites'\n",
    "4. 'http://www.fakenewsai.com/'\n",
    "5. 'https://morningconsult.com/2016/12/07/poll-majority-find-major-media-outlets-credible/'\n",
    "6. 'http://www.pewresearch.org/fact-tank/2014/10/30/which-news-organization-is-the-most-trusted-the-answer-is-complicated/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the fake news and true news list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fake_news = ['http://ABCnews.com.co','http://bizstandardnews.com','http://Bloomberg.ma',\n",
    "     'http://70news.wordpress.com','http://beforeitsnews.com', 'http://ddsnewstrend.com', \n",
    "     'http://thebostontribune.com/','http://americanfreepress.net/','http://www.bipartisanreport.com/',\n",
    "     'http://aurora-news.us/', 'http://conservativefighters.com/','http://conservativespirit.com/',\n",
    "     'http://conservative101.com/','http://DrudgeReport.com.co','http://NBCNews.com.co','http://TrueTrumpers.com',\n",
    "     'http://UndergroundNewsReport.com','http://washingtonpost.com.co','http://YourNewsWire.com','http://cnn.com.de/',\n",
    "     'http://rickwells.us/','http://thedcgazette.com/','http://donaldtrumppotus45.com/','http://24wpn.com',\n",
    "     'http://AmericanFlavor.news','http://AmericanPresident.co','http://AMPosts.com','http://BB4SP.com',\n",
    "     'http://BlueVisionPost.com','http://CivicTribune.com']\n",
    "\n",
    "true_news = ['https://www.wsj.com/','https://www.nytimes.com/','http://www.bbc.com/news',\n",
    "             'http://www.npr.org/sections/news/', 'http://www.reuters.com/', 'https://www.economist.com/',\n",
    "             'https://www.apnews.com/', 'http://www.cnn.com', 'http://www.foxnews.com/', \n",
    "             'http://www.politico.com/', 'http://www.nbcnews.com/', 'http://www.msnbc.com/', 'http://www.cbsnews.com/',\n",
    "             'http://www.huffingtonpost.com/','http://www.bloomberg.com/', 'http://abcnews.go.com/',\n",
    "             'http://www.aljazeera.com/news/', 'https://www.afp.com/en/news-hub', 'http://www.newyorker.com/',\n",
    "             'https://www.theguardian.com/', 'http://www.telegraph.co.uk/', 'http://www.zeit.de/english/index',\n",
    "             'http://www.chicagotribune.com/', 'http://www.freep.com/', 'http://www.bostonherald.com/',\n",
    "             'http://www.dailypress.com/', 'http://www.detroitnews.com/', 'https://www.ft.com/', \n",
    "             'http://www.ibtimes.com/', 'http://www.voanews.com/']\n",
    "\n",
    "\n",
    "\n",
    "print(\"We have {} fake news and {} true news organizations\".format(len(fake_news), len(true_news)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 news organizations per person per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyesoo_fake_news = ['http://ABCnews.com.co','http://Americannews.com','http://Americanoverlook.com', \n",
    "                    'http://Bighairynews.com','http://bizstandardnews.com','http://Bloomberg.ma',\n",
    "                    'http://70news.wordpress.com','http://beforeitsnews.com','http://Cap-news.com',\n",
    "                    'http://ddsnewstrend.com', 'http://thebostontribune.com/','http://americanfreepress.net/',\n",
    "                    'http://www.bipartisanreport.com/','http://aurora-news.us/', 'http://Clashdaily.com',\n",
    "                    'http://Conservativedailypost.com', 'http://Conservativeinfidel.com','http://Dailyheadlines.com',\n",
    "                    'http://DeadlyClear.wordpress.com', 'http://Donaldtrumpnews.co', 'http://Freedomdaily.com']\n",
    "\n",
    "\n",
    "hyesoo_true_news = ['https://www.wsj.com/','https://www.nytimes.com/','http://www.bbc.com/news',\n",
    "             'http://www.npr.org/sections/news/', 'http://www.reuters.com/', 'https://www.economist.com/',\n",
    "             'https://www.apnews.com/', 'http://www.cnn.com', 'http://www.foxnews.com/', \n",
    "             'http://www.politico.com/'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(fake_news).difference(set(hyesoo_fake_news)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import libraries and tools\n",
    "* downloading nltk can take some time\n",
    "* alternative of 'nltk spell check' is to use a libary called enchanted, but I failed to install it for some reasons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import newspaper \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nltk.download() # A window will pop up and need to click 'all' download, may hide behind your browser!\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words()) # Make english dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can somebody check this function please? It's passing  'if statement' even if it should not, is it due to try-except?\n",
    "\n",
    "* It still creates the raw data\n",
    "* However my intention to rule out 'n/a text' and 'non-english text', and to keep the shape (nX4) doesn't work somehow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Don't know why this one is not filtering the if statement conditions but still pass it !!!!\n",
    "\n",
    "def generate_raw_data_2(news_list, data_name):\n",
    "    col_names = [\"source\", \"title\", \"author\", \"text\"]\n",
    "    article_df = pd.DataFrame(columns = col_names)\n",
    "    \n",
    "    for news in news_list:\n",
    "        news_articles = newspaper.build(news, memoize_articles=False)\n",
    "        count = 0\n",
    "        for article in news_articles.articles:\n",
    "            try:\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                entry = pd.DataFrame([[news, article.title, article.authors, article.text]], columns=col_names)\n",
    "                text_vocab = set(w.lower() for w in entry.title if w.lower().isalpha()) \n",
    "                if (entry.text.notnull()[0]) and (len(text_vocab.difference(english_vocab) ) < 2):\n",
    "                    article_df = article_df.append(entry)\n",
    "                    count += 1\n",
    "            except:\n",
    "                pass\n",
    "         \n",
    "        print(\"The total number of \" + str(news_articles.brand) + \" articles is \", count)\n",
    "    article_df = article_df[col_names]\n",
    "    article_df.to_csv(data_name+\".csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative to the function above\n",
    "\n",
    "* a trial to call shape_data function in generate_raw_data function yielded the same faulty result \n",
    "* rather save the df as csv first, and call it again and fix it using shape_data function, and save it again.\n",
    "* It's a dirty way, any better idea?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def shape_data(df):\n",
    "    col_names = [\"source\", \"title\", \"author\", \"text\"]\n",
    "    df = df[df.text.notnull()]\n",
    "    df = df[col_names]    \n",
    "    for title in df.title:\n",
    "        text_vocab = set(w.lower() for w in title if w.lower().isalpha()) \n",
    "        if len(text_vocab.difference(english_vocab)) > 2:\n",
    "               df = df[df.title != title]              \n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_raw_data(news_list, data_name):\n",
    "    col_names = [\"source\", \"title\", \"author\", \"text\"]\n",
    "    article_df = pd.DataFrame(columns = col_names)\n",
    "    \n",
    "    for news in news_list:\n",
    "        news_articles = newspaper.build(news, memoize_articles=False)\n",
    "        count = 0\n",
    "        for article in news_articles.articles:\n",
    "            try:\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                entry = pd.DataFrame([[news_articles.brand, article.title, article.authors, article.text]], columns=col_names)\n",
    "                text_vocab = set(w.lower() for w in entry.title if w.lower().isalpha()) \n",
    "                article_df = article_df.append(entry)\n",
    "                count += 1\n",
    "            except:\n",
    "                pass\n",
    "         \n",
    "        print(\"The total number of \" + str(news_articles.brand) + \" articles is \", count)    \n",
    "    article_df.to_csv(data_name+\".csv\")\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From here on, run the code under your name and it should (hopefully) generate 2 csv files per person\n",
    "\n",
    "* Make sure to fix the structure/contents using shape_data function or any other preferred script of yours!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyesoo\n",
    "\n",
    "# generate_raw_data(hyesoo_fake_news, 'hyesoo_fake_news_rawdata')\n",
    "# df = pd.read_csv('hyesoo_fake_news_rawdata.csv\")\n",
    "# shape_data(df).to_csv('hyesoo_fake_news_rawdata.csv\")\n",
    "generate_raw_data(hyesoo_true_news, 'hyesoo_true_news_rawdata')\n",
    "# df = pd.read_csv('hyesoo_true_news_rawdata.csv\")\n",
    "# shape_data(df).to_csv('hyesoo_true_news_rawdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_raw_data_fake(news_list, data_name):\n",
    "    col_names = [\"source\", \"title\", \"author\", \"text\"]\n",
    "    article_df = pd.DataFrame(columns = col_names)\n",
    "    final_news_list = []\n",
    "    final_article_number = {}\n",
    "    news_list = (x for x in news_list)\n",
    "    while len(final_news_list) < 10:\n",
    "        news = next(news_list)\n",
    "        try:\n",
    "            news_articles = newspaper.build(news, memoize_articles=False)\n",
    "            final_news_list += [news_articles]\n",
    "        except:\n",
    "            pass\n",
    "     \n",
    "    for news_articles in final_news_list:\n",
    "        count = 0\n",
    "        for article in news_articles.articles:\n",
    "            try:\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                entry = pd.DataFrame([[news_articles.brand, article.title, article.authors, article.text]], columns=col_names)\n",
    "                text_vocab = set(w.lower() for w in entry.title if w.lower().isalpha()) \n",
    "                article_df = article_df.append(entry)\n",
    "                count += 1\n",
    "            except:\n",
    "                pass\n",
    "         \n",
    "        print(\"The total number of \" + str(news_articles.brand) + \" articles is \", count) \n",
    "        final_article_number[news_articles.brand] = count \n",
    "    article_df.to_csv(data_name+\".csv\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = ['h','a','l']\n",
    "B = (x for x in A)\n",
    "next(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_raw_data_fake(hyesoo_fake_news, 'hyesoo_fake_news_rawdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_raw_data_fake2(news_list, data_name):\n",
    "    col_names = [\"source\", \"title\", \"author\", \"text\"]\n",
    "    article_df = pd.DataFrame(columns = col_names)\n",
    "    final_news_list = []\n",
    "    final_article_number = {}\n",
    "    total_count = 0\n",
    "#     news_list = (x for x in news_list)\n",
    "#     news = next(news_list)\n",
    "    for news in news_list:\n",
    "        try:\n",
    "            news_articles = newspaper.build(news, memoize_articles=False)\n",
    "            final_news_list += [news_articles]\n",
    "        except:\n",
    "            pass\n",
    "    print (final_news_list, len(final_news_list))\n",
    "    for news_articles in final_news_list:\n",
    "        if total_count < 1100:\n",
    "            count = 0\n",
    "            for article in news_articles.articles:\n",
    "                try:\n",
    "                    article.download()\n",
    "                    article.parse()\n",
    "                    entry = pd.DataFrame([[news_articles.brand, article.title, article.authors, article.text]], columns=col_names)\n",
    "                    text_vocab = set(w.lower() for w in entry.title if w.lower().isalpha()) \n",
    "                    article_df = article_df.append(entry)\n",
    "                    count += 1\n",
    "                    total_count += 1\n",
    "                except:\n",
    "                    pass\n",
    "            print(\"The total number of \" + str(news_articles.brand) + \" articles is \", count) \n",
    "            final_article_number[news_articles.brand] = count\n",
    "        else:\n",
    "            pass\n",
    "    print(final_article_number)\n",
    "    article_df.to_csv(data_name+\".csv\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_raw_data_fake2(hyesoo_fake_news, 'hyesoo_fake_news_rawdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "E = 20+16+59+59+212+287+47+181+21+29+132+62\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "truenews_df = pd.read_csv('hyesoo_true_news_rawdata.csv')\n",
    "truenews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(list(truenews_df.source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "truenews_df = truenews_df[truenews_df['text'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_names = [\"source\", \"title\", \"author\", \"text\"]\n",
    "truenews_df = truenews_df[col_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "truenews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for t in truenews_df.text[0:10]:\n",
    "    print(t, len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for t in truenews_df.text:\n",
    "    if len(t)>1000:\n",
    "        print(t, len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
