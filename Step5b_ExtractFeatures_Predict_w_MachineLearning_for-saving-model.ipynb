{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This file does below:\n",
    "1. Import dataset and generate/add features\n",
    "2. Run different classifier models and predict the authenticity of a randomly selected article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Import libraries & Packages\n",
    "\n",
    "# Data structure and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import dill as pickle\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data (balanced_data.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>authenticity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2941</th>\n",
       "      <td>Evangelicals Pray For President In W.H. – Demo...</td>\n",
       "      <td>Evangelicals Pray For President In W.H. – Demo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2942</th>\n",
       "      <td>Vaccines: Good or Bad?, Part 8</td>\n",
       "      <td>Vaccines: Good or Bad?, Part 8\\n\\nThe more we ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>Father PUNISHES 10-Year-Old Daughter by Strand...</td>\n",
       "      <td>Christopher Charles Watson, from Kingman, Ariz...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2944</th>\n",
       "      <td>Delta purchases 10% stake in Air France-KLM</td>\n",
       "      <td>Delta Air Lines (DAL) and China Eastern Airlin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945</th>\n",
       "      <td>Can forgotten rubella children of the '60s hol...</td>\n",
       "      <td>Brooklyn, New York (CNN) One side of the bedro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "2941  Evangelicals Pray For President In W.H. – Demo...   \n",
       "2942                     Vaccines: Good or Bad?, Part 8   \n",
       "2943  Father PUNISHES 10-Year-Old Daughter by Strand...   \n",
       "2944        Delta purchases 10% stake in Air France-KLM   \n",
       "2945  Can forgotten rubella children of the '60s hol...   \n",
       "\n",
       "                                                   text  authenticity  \n",
       "2941  Evangelicals Pray For President In W.H. – Demo...             1  \n",
       "2942  Vaccines: Good or Bad?, Part 8\\n\\nThe more we ...             1  \n",
       "2943  Christopher Charles Watson, from Kingman, Ariz...             1  \n",
       "2944  Delta Air Lines (DAL) and China Eastern Airlin...             0  \n",
       "2945  Brooklyn, New York (CNN) One side of the bedro...             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.join('data', 'balanced_data.csv')\n",
    "total_df = pd.read_csv(path, usecols=[3,5,6])\n",
    "total_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import additional features\n",
    "* Author exists (1) / not exists (0)\n",
    "* captital rates of title text, normalized to the average of caprate_title\n",
    "* Rate of exaggerating punctuations [!,?,:,-] in title text, normalized to the average of exagg_puct_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>caprate_title</th>\n",
       "      <th>exagg_puct_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.065457</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.891909</td>\n",
       "      <td>9.623847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.251912</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.561081</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.745185</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   author  caprate_title  exagg_puct_title\n",
       "0       1       1.065457          0.000000\n",
       "1       0       1.891909          9.623847\n",
       "2       0       1.251912          0.000000\n",
       "3       0       0.561081          0.000000\n",
       "4       1       0.745185          0.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.join('data', 'additional_features.csv')\n",
    "addfeat_df = pd.read_csv(path, usecols=[1,2,3])\n",
    "addfeat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the whole dataset into train, cv, test datasets \n",
    "\n",
    "* X_( ) = text of article body, before processed\n",
    "* af_( ) = additional features, already processed\n",
    "* Combining X_ and af_ first, and split the data together, and separate them again so that text can be processed separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = total_df['authenticity']\n",
    "addfeat_df['text'] = total_df.text\n",
    "addfeat_df.head()\n",
    "Xaf = addfeat_df\n",
    "Xaf_train, Xaf_cv, Y_train, Y_cv = train_test_split(Xaf, Y, test_size=0.2, random_state=42)\n",
    "X_train, af_train = Xaf_train.text, Xaf_train.drop('text', axis = 1)\n",
    "X_cv, af_cv = Xaf_cv.text, Xaf_cv.drop('text', axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate features from article body text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_stemmer(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # option to include punctuation or not\n",
    "    #tokens = [i for i in tokens if i not in string.punctuation]\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "pickle.dump(stem_tokens, open('stem_tokens.pkl', 'wb'), protocol=2)\n",
    "pickle.dump(tokenize_stemmer, open('tokenize_stemmer.pkl', 'wb'), protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Use TF-IDF to generate features for the text body of news¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "my_additional_stop_words = ['abc', 'nbc', 'npr', 'cnn', 'reuters', 'fox', \n",
    "                            'bbc', 'cbs', 'newyorker', 'msnbc', 'politico', 'nytimes',\n",
    "                            'sputniknews', 'lastdeplorables', 'readconservatives', 'wordpress',\n",
    "                            'infostormer', 'Americannews', 'ABCnews', 'nationonenews', 'majorthoughts',\n",
    "                            'interestingdailynews', 'donaldtrumppotus45', 'newsbbc', 'beforeitsnews', \n",
    "                            'krbcnews', 'Conservativedailypost', 'thedcgazette', 'Americanoverlook', \n",
    "                            'CivicTribune', 'openmagazines', 'politicono', 'bizstandardnews', 'president45donaldtrump',\n",
    "                            'nbc', 'AmericanFlavor', 'prntly', 'bipartisanreport', 'americanfreepress', \n",
    "                            'ladylibertysnews', 'politicalo', 'now8news', '24wpn', 'pamelageller', \n",
    "                            'ddsnewstrend', 'Bighairynews', 'redcountry', 'newswithviews', 'Clashdaily', \n",
    "                            'aurora-news', 'nephef', 'local31news', 'realnewsrightnow', 'reagancoalition',\n",
    "                            'reuter', 'sputnik', \n",
    "                            'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday' ]\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize_stemmer,stop_words=my_stop_words,ngram_range=(1, 2))\n",
    "tfidf.fit(X_train)\n",
    "pickle.dump(tfidf, open('tfidf.sav', 'wb'), protocol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2356, 630691)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tot = tfidf.transform(X_train)\n",
    "X_cv_tot = tfidf.transform(X_cv)\n",
    "\n",
    "X_train_tot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, SelectKBest, f_classif, chi2, SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_selected shape:  (2356, 2535)\n",
      "X_cv_selected shape:  (590, 2535)\n"
     ]
    }
   ],
   "source": [
    "# select top 10% features\n",
    "# selector = SelectPercentile(f_classif, percentile = 10) \n",
    "\n",
    "# select top 25000 features\n",
    "#selector = SelectKBest(chi2, k = 10000) \n",
    "\n",
    "\n",
    "# select from model\n",
    "lsvc = LinearSVC(C=9000, penalty=\"l1\", dual=False)#.fit(X_train_tot, Y_train)\n",
    "selector = SelectFromModel(lsvc, prefit=False)\n",
    "\n",
    "selector.fit(X_train_tot, Y_train)\n",
    "pickle.dump(selector, open('selector.sav', 'wb'), protocol=3)\n",
    "X_train_selected = selector.transform(X_train_tot)\n",
    "X_cv_selected = selector.transform(X_cv_tot)\n",
    "\n",
    "print(\"X_train_selected shape: \", X_train_selected.shape)\n",
    "print(\"X_cv_selected shape: \", X_cv_selected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set train, test, cv data (choose one method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "def concat_3features(df):\n",
    "    author = np.array([[x] for x in df.author])\n",
    "    caprate = np.array([[x] for x in df.caprate_title])\n",
    "    exagg =  np.array([[x] for x in df.exagg_puct_title])\n",
    "    return  np.concatenate((author, caprate, exagg), axis=1)\n",
    "\n",
    "# add additional feature names\n",
    "feat_names_add = ['author', 'title_capitalization', 'exclamation_in_title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use selected and additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sel_add shape:  (2356, 2538)\n",
      "X_cv_sel_add shape:  (590, 2538)\n"
     ]
    }
   ],
   "source": [
    "X_train_sel_add = sp.sparse.hstack((X_train_selected, concat_3features(af_train)))\n",
    "X_cv_sel_add = sp.sparse.hstack((X_cv_selected, concat_3features(af_cv)))\n",
    "\n",
    "print(\"X_train_sel_add shape: \", X_train_sel_add.shape)\n",
    "\n",
    "print(\"X_cv_sel_add shape: \", X_cv_sel_add.shape)\n",
    "\n",
    "X_train_dtm = X_train_sel_add\n",
    "X_cv_dtm = X_cv_sel_add\n",
    "\n",
    "\n",
    "# path_1 = os.path.join('data','X_train_sel_add.csv')\n",
    "# X_train_sel_add.to_csv(path_1)\n",
    "# path_2 = os.path.join('data','Y_train_sel_add.csv')\n",
    "# Y_train_sel_add.to_csv(path_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test different machine learning classifier models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# validation test & Learning curve\n",
    "\n",
    "def valid_test(model, param, param_candidates):\n",
    "    train_scores, valid_scores = validation_curve(model, X_cv_dtm, Y_cv, param, param_candidates)\n",
    "    avg_ts, avg_vs = train_scores.mean(axis = 1), valid_scores.mean(axis = 1)\n",
    "    sd_ts, sd_vs = train_scores.std(axis = 1), valid_scores.std(axis = 1)\n",
    "    vs_max_ix = np.argmax(avg_vs)\n",
    "    best = param_candidates[vs_max_ix]\n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_C = valid_test(LogisticRegression(), \"C\", np.logspace(2, 4, 10))\n",
    "LR = LogisticRegression(C = best_C)\n",
    "# #LR = LogisticRegression()\n",
    "LR.fit(X_train_dtm, Y_train)\n",
    "# Y_test_pred_LR = LR.predict(X_test_dtm)\n",
    "pickle.dump(LR, open('LR.sav', 'wb'), protocol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
