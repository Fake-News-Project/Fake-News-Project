{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# twitter_selenium_scraper from the link below\n",
    "\n",
    "* https://github.com/rjshanahan/twitter_scraper/blob/master/twitter_selenium_scraper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter webscraper for specific pages\n",
    "\n",
    "Python web scraper using Selenium and BeautifulSoup modules to extract text from various Twitter pages.\n",
    "\n",
    "The program uses Selenium (and ChromeDriver) to automate user behaviour within a browser session to load a specific Twitter page (no login) and load data from dynamic scrolling. Once the pages are rendered the HTML is extracted and sieved through BeautifulSoup. Note: it will continue scraping until 1) end of feed is reached, 2) manual interrupt by killing the connection.\n",
    "\n",
    "This program will extract the following and output to a CSV file with punctuation and other non-text characters removed:\n",
    "\n",
    "full tweet text from each Twitter page\n",
    "date\n",
    "header\n",
    "url\n",
    "user name\n",
    "popularity metrics (string containing retweets/favourites)\n",
    "like_fave: integer value for number of times 'favorited'\n",
    "share_rtwt: integer value for number of times 'retweeted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#4 October 2015\n",
    "#Richard Shanahan\n",
    "#this code scrapes Twitter pages WITHOUT login\n",
    "#it can handle dynamic scroll/load pages - it will continue scraping until 1) end of feed is reached, 2) manual interrupt by killing the connection\n",
    "#NOTE: this code uses Chrome WebDriver with Selenium\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "import pprint as pp\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "path_to_chromedriver = '/Users/YOURNAME/chromedriver'            # change path as needed\n",
    "browser = webdriver.Chrome(executable_path = path_to_chromedriver)\n",
    "\n",
    "\n",
    "#url = raw_input(['Enter your Twitter page URL: ']) + '/'\n",
    "\n",
    "\n",
    "#function to handle dynamic page content loading - using Selenium\n",
    "def twt_scroller(url):\n",
    "\n",
    "    browser.get(url)\n",
    "    \n",
    "    #define initial page height for 'while' loop\n",
    "    lastHeight = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        #define how many seconds to wait while dynamic page content loads\n",
    "        time.sleep(2)\n",
    "        newHeight = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        if newHeight == lastHeight:\n",
    "            break\n",
    "        else:\n",
    "            lastHeight = newHeight\n",
    "            \n",
    "    html = browser.page_source\n",
    "\n",
    "    return html\n",
    "\n",
    "\n",
    "    \n",
    "#function to handle/parse HTML and extract data - using BeautifulSoup    \n",
    "def blogxtract(url):\n",
    "    \n",
    "    #regex patterns\n",
    "    problemchars = re.compile(r'[\\[=\\+/&<>;:!\\\\|*^\\'\"\\?%$@)(_\\,\\.\\t\\r\\n0-9-â€”\\]]')\n",
    "    prochar = '[(=\\-\\+\\:/&<>;|\\'\"\\?%#$@\\,\\._)]'\n",
    "    crp = re.compile(r'MoreCopy link to TweetEmbed Tweet|Reply')\n",
    "    wrd = re.compile(r'[A-Z]+[a-z]*')\n",
    "    dgt = re.compile(r'\\d+')\n",
    "    url_finder = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    retweet = re.compile(r\"(?<=Retweet:)(.*)(?=', u'R)\")\n",
    "    fave = re.compile(r\"(?<=Like:)(.*)(?=', u'Liked)\")\n",
    "\n",
    "    blog_list = []\n",
    "     \n",
    "    #set to global in case you want to play around with the HTML later   \n",
    "    global soup    \n",
    "    \n",
    "    #call dynamic page scroll function here\n",
    "    soup = BeautifulSoup(twt_scroller(url), \"html.parser\")\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        for i in soup.find_all('li', {\"data-item-type\":\"tweet\"}):\n",
    "            \n",
    "    \n",
    "            user = (i.find('span', {'class':\"username js-action-profile-name\"}).get_text() if i.find('span', {'class':\"username js-action-profile-name\"}) is not None else \"\")\n",
    "            link = ('https://twitter.com' + i.small.a['href'] if i.small is not None else \"\")\n",
    "            date = (i.small.a['title'] if i.small is not None else \"\")\n",
    "            popular = (i.find('div', {'class': \"ProfileTweet-actionList js-actions\"}).get_text().replace('\\n','') if i.find('div', {'class': \"ProfileTweet-actionList js-actions\"}) is not None else \"\")\n",
    "            text = (i.p.get_text().lower().encode('ascii', 'ignore').strip().replace('\\n',' ').replace(\"'\",'') if i.p is not None else \"\")\n",
    "            popular_text = [i + ':' + j  if len(dgt.findall(popular)) != 0 else '' for i, j in zip(wrd.findall(crp.sub('', popular)), dgt.findall(popular))]\n",
    "\n",
    "            \n",
    "            #build dictionary\n",
    "            blog_dict = {\n",
    "            \"header\": \"twitter_hashtag_\" + url.rsplit('/',2)[1],\n",
    "            \"url\": link,\n",
    "            \"user\": user,\n",
    "            \"date\": date,\n",
    "            \"popular\": popular_text,\n",
    "            #before text is stored URLs are removed - note: hash symbol is maintained to indicate hashtag term\n",
    "            \"blog_text\": problemchars.sub(' ', url_finder.sub('', text)),\n",
    "            \"like_fave\": (int(''.join(fave.findall(str(popular_text)))) if len(fave.findall(str(popular_text))) > 0 else ''),\n",
    "            \"share_rtwt\": (int(''.join(retweet.findall(str(popular_text)))) if len(retweet.findall(str(popular_text))) > 0 else '')\n",
    "            }\n",
    "        \n",
    "            blog_list.append(blog_dict)\n",
    "            \n",
    "    #error handling  \n",
    "    except (AttributeError, TypeError, KeyError, ValueError):\n",
    "        print \"missing_value\"\n",
    "        \n",
    "            \n",
    "    #call csv writer function and output file\n",
    "    writer_csv_3(blog_list)\n",
    "    \n",
    "    return pp.pprint(blog_list[0:2])\n",
    "\n",
    "    \n",
    "    \n",
    "#function to write CSV file\n",
    "def writer_csv_3(blog_list):\n",
    "    \n",
    "    #uses group name from URL to construct output file name\n",
    "    file_out = \"twitter_hashtag_{page}.csv\".format(page = url.rsplit('/',2)[1])\n",
    "    \n",
    "    with open(file_out, 'w') as csvfile:\n",
    "\n",
    "        writer = csv.writer(csvfile, lineterminator='\\n', delimiter=',', quotechar='\"')\n",
    "    \n",
    "        for i in blog_list:\n",
    "            if len(i['blog_text']) > 0:\n",
    "                newrow = i['header'], i['url'], i['user'], i['date'], i[\"popular\"], i['blog_text'], i[\"like_fave\"], i[\"share_rtwt\"]\n",
    "\n",
    "                writer.writerow(newrow)                     \n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    \n",
    "#tip the domino\n",
    "if __name__ == \"__main__\":\n",
    "    blogxtract(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
